{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrksvwCFHghPjcWWn2JDd7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gut-puncture/gpt-2-shakespeare/blob/main/gpt_2_shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfll-jx5PW0I",
        "outputId": "0fdec911-33cf-42cf-a894-1e20904ee0d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-29 17:33:42--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-07-29 17:33:42 (39.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "dWXmf0wN9fq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training params\n",
        "batch_size = 32 #8 inputs given to the model in one training step\n",
        "block_size = 8 #maximum context length for prediction or chunk size\n",
        "max_iters = 10000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtBHCSBY9Zia",
        "outputId": "9c0d473f-df72-41ec-bf61-930ddb798859"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79d9803ab230>"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read text to inspect\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "7AnSb5kiQIsZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwZ7ojfTQUsn",
        "outputId": "0898ef00-9d19-441d-a95e-8d93488d3474"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(len(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_tiOdY1QiXP",
        "outputId": "795b587e-f511-466d-818b-2c88d6985cdc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#char and integer token id lookup dicts\n",
        "s_to_i = {}\n",
        "i_to_s = {}\n",
        "for pair in enumerate(chars):\n",
        "  pair = list(pair)\n",
        "  s_to_i[pair[1]] = pair[0]\n",
        "  i_to_s[pair[0]] = pair[1]"
      ],
      "metadata": {
        "id": "9n63i_UaRM34"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(string):\n",
        "  return [s_to_i[char] for char in string]\n",
        "\n",
        "def decode(integer_list):\n",
        "  return ''.join([i_to_s[integer] for integer in integer_list])"
      ],
      "metadata": {
        "id": "pN-lCc0jRuQA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating tensor for entire text\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "data.shape, data.dtype\n",
        "data[:100]"
      ],
      "metadata": {
        "id": "fcvSTrQtRzBc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train-test split\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "GZyx_UcNRzdk"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "block_size = 8 #max size of chunk for training\n",
        "batch_size = 4\n",
        "\n",
        "def get_batch(split):\n",
        "  #generate a small batch of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  index = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in index:\n",
        "    x.append(data[i:i+block_size])\n",
        "    y.append(data[i+1:i+block_size+1])\n",
        "  x = torch.stack(x)\n",
        "  y = torch.stack(y)\n",
        "  return x, y\n",
        "\n",
        "#FOR UNDERSTANDING: training example\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b][:t+1]\n",
        "    target = yb[b,t]\n",
        "    print(context, target)"
      ],
      "metadata": {
        "id": "AXlildlYTMcv"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X,Y)\n",
        "      losses[k] = loss.item() #.item simply gets the loss value out of the loss object which is cross_entropy object\n",
        "    out[split] = losses.mean() #diff losses for train and validation set\n",
        "  model.train()\n",
        "  return out\n",
        "\n"
      ],
      "metadata": {
        "id": "bTANvkT5-xjQ"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#putting the input in a Bigram Language Model\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    #idx and targets are are both (B,T) tensor of integers\n",
        "    logits = self.token_embedding_table(idx) #shape is (Batch, block/chunk size, vocab size);\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "  #idx is of the shape (B,T) but the T is not necessarily equal to the batch size (8).\n",
        "  #this is just a way of saying that the input taken for generating text by the model is exactly like the input taken for training i.e.\n",
        "  #a bunch of tokens in a sequence\n",
        "    for i in range(max_new_tokens):\n",
        "      logits, loss = self(idx)\n",
        "\n",
        "      logits = logits[:, -1, :] #becomes [B,C] since we only extract the logits of the last token in the input\n",
        "      #apply softmax\n",
        "      probs = F.softmax(logits, dim=-1) #becomes [B, C]\n",
        "      #sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples =1) #(B,1) as one of the token ids is selected as per the probs\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb,yb)\n",
        "print(logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT953BHGiZXQ",
        "outputId": "8b71cef5-e71a-4913-c9e0-1e654ae2ca4f"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer object\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "QWv0spLSkemX"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "max_iters = 10000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  #sample text\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UWc24YK4TTl",
        "outputId": "a32567c0-2445-4ce3-83bd-e00711430d41"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.474918842315674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long) #one batch with one token long batch size and the token is \\n since that is token id 0\n",
        "print(decode(m.generate(idx, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "id": "VycFo4q58kvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AAb5XFPs_i39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Self Attention"
      ],
      "metadata": {
        "id": "paadmLc-AIO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#self_attention mask understanding example\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4,8,2 #Batches, Context/Block/Chunk length or the time dimension, Channels or information stored for each token\n",
        "x = torch.randn(B,T,C)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "24agDKTLAJoO",
        "outputId": "7f3a1d9a-9471-45e2-a7c8-2953b7aebcf0"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.3596, -0.9152],\n",
              "         [ 0.6258,  0.0255],\n",
              "         [ 0.9545,  0.0643],\n",
              "         [ 0.3612,  1.1679],\n",
              "         [-1.3499, -0.5102],\n",
              "         [ 0.2360, -0.2398],\n",
              "         [-0.9211,  1.5433]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.2858,  0.9651],\n",
              "         [-2.0371,  0.4931],\n",
              "         [ 1.4870,  0.5910],\n",
              "         [ 0.1260, -1.5627],\n",
              "         [-1.1601, -0.3348],\n",
              "         [ 0.4478, -0.8016],\n",
              "         [ 1.5236,  2.5086]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 1.0101,  0.1215],\n",
              "         [ 0.1584,  1.1340],\n",
              "         [-1.1539, -0.2984],\n",
              "         [-0.5075, -0.9239],\n",
              "         [ 0.5467, -1.4948],\n",
              "         [-1.2057,  0.5718],\n",
              "         [-0.5974, -0.6937]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.3514, -0.2759],\n",
              "         [-1.5108,  2.1048],\n",
              "         [ 2.7630, -1.7465],\n",
              "         [ 1.4516, -1.5103],\n",
              "         [ 0.8212, -0.2115],\n",
              "         [ 0.7789,  1.5333],\n",
              "         [ 1.6097, -0.4032]]])"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros(B,T,C)\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1] #(t,C) #this is a rudimentary \"bag of words\" example where we average out the logits for each token before a token. real attention blocks will not average\n",
        "    xbow[b,t] = torch.mean(xprev,0)\n",
        "\n",
        "print(xbow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oqKydSO7A_lp",
        "outputId": "1dcb32a8-41f5-4ca5-8e69-d28958346704"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we want to ensure that the tokens can only look at the tokens which came before them to get context.\n",
        "#When we're serving a string of text to the model, we will ensure that each token can only look at the tokens befor it.\n",
        "#This is done using some mathematical insights:\n",
        "#1. we can take a triangular matrix with half of the matrix being zeroes and the other half filled.\n",
        "#   Then we take the weights of each token and create a mask where we mark each element in the upper triangle as -inf\n",
        "#   Applying softmax to this"
      ],
      "metadata": {
        "id": "bGlx_9_tFu9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones((T,T)))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim = -1)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbsfZP49CDQc",
        "outputId": "4e3f1b04-5760-4329-d6fe-70e1e325f996"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KjgfDJhRE-PU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}